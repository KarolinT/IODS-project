# Week 2. Linear regression

```{r}
date()
```

First, use read in the dataset we use and name it learning2014.
```{r}
learning2014=read.table("learning2014.csv", header= TRUE, sep=";")
```


```{r}
dim(learning2014)
```
The dataset has 166 rows and 7 columns, meaning
166 participants answered about their learning and we use 7 variables.

```{r}
names(learning2014)
```
*gender*, *age*


#graphical overview, summaries of variables
```{r}
summary(learning2014)

library(GGally)
library(ggplot2)

# create a more advanced plot matrix with ggpairs()
p <- ggpairs(learning2014, mapping = aes(color=gender, alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))

# draw the plot
p
```



library(ggplot2)
qplot(attitude, points, data = learning2014) + geom_smooth(method = "lm")



```{r}
my_model2 <- lm(points ~ attitude + stra+surf, data = learning2014)
summary(my_model2)
```


```{r}
my_model3 <- lm(points ~ attitude+stra, data = learning2014)
summary(my_model3)
```

stra is also not statistically significant, but I keep it in the model.

Regression model has its assumpions.
Assumptions have to fit the reality. If this isn't true, the model doesn't describe the phenomenon of interest.

Errors have to be normally distributed, constant.
```{r}
par(mfrow = c(2,2))
plot(my_model3, which=c(1,2,5))
```

Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage

- Describe your work and results clearly. 
- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
- Assume the reader has no previous knowledge of your data or the more advanced methods you are using.

Here we go again...
